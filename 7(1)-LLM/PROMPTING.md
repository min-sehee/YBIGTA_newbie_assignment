# Prompting 결과 보고서

## ✅ Prompting Accuracy

| Prompt Type    | 0-shot | 3-shot | 5-shot |
|----------------|--------|--------|--------|
| Direct Prompting | 26.00% | 20.00% | 22.00% |
| CoT Prompting     | 66.00% | 70.00% | 64.00% |
| My Prompting      | 72.00% | 70.00% | 68.00% |

---

## 📌 CoT Prompting이 Direct Prompting보다 좋은 이유

Direct Prompting은 단순히 예시 없이 문제와 정답만을 나열하기 때문에  
모델이 **문제 해결 과정을 추론하거나 복잡한 연산을 수행하는 데 어려움이 있다.**  
반면, Chain-of-Thought(COT) 방식은 각 문제에 대해 **풀이 과정을 단계별로 보여줌으로써**,  
모델이 문제의 구조를 파악하고 정확한 정답을 도출할 수 있도록 돕는다.

특히 GSM8K 같은 reasoning 중심 문제에선, **"정답만 보여주는" 방식보다 "어떻게 푸는지 보여주는" 방식이 훨씬 효과적**이다.

---

## 📌 My Prompting 기법이 CoT보다 더 좋은 이유

My Prompting은 CoT의 장점을 유지하면서,  
**추론 안정성을 높이기 위해 검산 과정을 도입했다.**

우선, 프롬프트에서 문제를 단계별로 풀도록 유도한 다음,  
추가로 한 번 더 정답을 확인하게 만드는 "Double-check" 지시를 넣어  
모델이 계산 실수를 줄일 수 있도록 했다.  
또한 정답은 항상 `'####'` 뒤에 출력하도록 유도해서  
정답 추출 과정에서도 혼동이 없도록 설계했다.

예시 구성도 모두 같은 형식(풀이 → 검산 → 정답)으로 통일해  
모델이 예시에서 학습할 흐름을 쉽게 따라갈 수 있도록 했고,  
그 결과 shot 수가 늘어나도 성능이 크게 흔들리지 않고 안정적으로 나왔다.

실제로 실험 결과에서도 My Prompting은 CoT와 비슷하거나 더 나은 성능을 보였고,  
특히 0-shot에서는 CoT보다도 높은 정확도를 기록했다.
